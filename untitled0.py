# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GRvTSZKLMzrdocp1rEoDwOE_Icecpbrg
"""

import pandas as pd

df = pd.read_csv('/content/loan_prediction.csv')

df.head()

"""Step 1: Understand the Data"""

# shape of the dataset
print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

# summary of the dataset
df.info()

# quick stats for numeric and object columns
print(df.describe(include='all'))

"""In this step, always look for:

1.Missing values
2.Categorical variables
3.Data types that might be wrong
4.High cardinality in string columns
5.Outliers or zero/negative values

Step 2: Handle Missing Values
"""

# count of missing values
missing = df.isnull().sum()
missing = missing[missing > 0].sort_values(ascending=False)
print(missing)

"""Here’s how to deal with the missing values one by one:"""

# fill categorical columns with mode
cat_cols = df.select_dtypes(include='object').columns
for col in cat_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].mode()[0], inplace=True)

# fill numerical columns with median
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in num_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].median(), inplace=True)

"""The median is robust to outliers, and the mode is perfect for majority categories in classification tasks.

Step 3: Fix Data Types

However, our dataset doesn’t require this step. But, sometimes numeric fields are stored as objects, or categorical fields are not cast properly. So, here’s how to fix data types if any:
"""

# example: convert 'LoanAmount' to float
df['LoanAmount'] = df['LoanAmount'].astype(float)

# convert categorical variables to category type
for col in cat_cols:
    df[col] = df[col].astype('category')

"""This helps with memory optimization and model compatibility.

Step 4: Standardize Text Columns

Our dataset doesn’t require this step as well. But this step is necessary when you have extra spaces in the text columns:
"""

# strip whitespace and convert to consistent case
for col in cat_cols:
    df[col] = df[col].str.strip().str.lower()

"""Step 5: Handle Outliers

This step is optional in the data cleaning part. You can either handle outliers after exploring the data in detail or remove the outliers if the problem you are solving is sensitive to outliers.

For numerical columns like LoanAmount or ApplicantIncome, we can cap/floor them based on percentiles:
"""

# capping outliers at 99th percentile
for col in ['LoanAmount', 'ApplicantIncome']:
    upper = df[col].quantile(0.99)
    df[col] = df[col].apply(lambda x: upper if x > upper else x)

"""Once cleaned, you should save it for further steps:"""

df.to_csv('cleaned_loan_data.csv', index=False)

"""```
 Final Pipeline Function
```

Here’s how to modularize everything into a reusable function:
"""

def clean_loan_data(df):
    # fill missing values
    for col in df.select_dtypes(include='object'):
        df[col].fillna(df[col].mode()[0], inplace=True)
    for col in df.select_dtypes(include=['int64', 'float64']):
        df[col].fillna(df[col].median(), inplace=True)

    # type conversions
    for col in df.select_dtypes(include='object'):
        df[col] = df[col].astype('category')
        df[col] = df[col].str.strip().str.lower()

    # outlier capping
    for col in ['LoanAmount', 'ApplicantIncome']:
        if col in df.columns:
            upper = df[col].quantile(0.99)
            df[col] = df[col].apply(lambda x: upper if x > upper else x)

    return df

"""Here’s how to use this function:"""

df = pd.read_csv('/content/loan_prediction.csv')
df_clean = clean_loan_data(df)

"""So, this is how to build a data cleaning pipeline using Pandas."""